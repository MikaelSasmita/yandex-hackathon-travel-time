{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65ed347",
   "metadata": {},
   "source": [
    "# Yandex Hackathon Indonesia - Final Submission Notebook\n",
    "\n",
    "**Participant:** mikaelradityas@gmail.com  \n",
    "**Model:** SGDRegressor with Huber Loss  \n",
    "**Description:** This notebook contains a complete pipeline for the competition task, including preprocessing, feature engineering, model training, and submission generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b130e73",
   "metadata": {},
   "source": [
    "### 1. Import Library\n",
    "All required Python libraries for data handling, preprocessing, modeling, and exporting submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12144e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9cf12",
   "metadata": {},
   "source": [
    "### 2. Load Dataset\n",
    "Load the training and test data provided in the competition. No additional external data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5471f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "train = pd.read_csv(\"train_sample.csv\")\n",
    "test = pd.read_csv(\"test_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b5f13",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "Convert data types if necessary and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03176f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric\n",
    "for col in ['vehicle_density', 'population_density']:\n",
    "    train[col] = pd.to_numeric(train[col], errors='coerce')\n",
    "    test[col] = pd.to_numeric(test[col], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bfb430",
   "metadata": {},
   "source": [
    "### 4. Transform Target & Fitur\n",
    "Apply log transformation to skewed features (`event_count`) and to the target (`travel_time`) for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f039952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform event_count\n",
    "train['event_count'] = np.log1p(train['event_count'])\n",
    "test['event_count'] = np.log1p(test['event_count'])\n",
    "\n",
    "# Log transform target\n",
    "y_train = np.log1p(train['travel_time'])\n",
    "X_train = train.drop(columns='travel_time')\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b19a4",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering\n",
    "Add meaningful features:  \n",
    "- `route`: combining start and end point  \n",
    "- `same_area`: binary indicator whether start and end are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cd62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'route' and 'same_area' features\n",
    "for df in [X_train, X_test]:\n",
    "    df['route'] = df['start_point'] + '->' + df['end_point']\n",
    "    df['same_area'] = (df['start_point'] == df['end_point']).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59c9fe",
   "metadata": {},
   "source": [
    "### 6. Preprocessing Pipelines\n",
    "Build preprocessing pipelines using `ColumnTransformer` to scale and encode the features appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309d7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "cat_cols = ['time_of_day', 'day_of_week', 'weather', 'route']\n",
    "num_std_cols = ['historical_delay_factor', 'public_transport_availability', 'is_holiday', 'same_area']\n",
    "num_mm_cols = ['traffic_condition', 'vehicle_density', 'population_density', 'event_count']\n",
    "\n",
    "# Pipelines\n",
    "cat_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "num_std_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "num_mm_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', cat_transformer, cat_cols),\n",
    "    ('num_std', num_std_transformer, num_std_cols),\n",
    "    ('num_mm', num_mm_transformer, num_mm_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b740cb7",
   "metadata": {},
   "source": [
    "### 7. Model Definition & Pipeline\n",
    "Use `SGDRegressor` with Huber loss and carefully tuned hyperparameters. Model is wrapped in a full pipeline with preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604069a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', SGDRegressor(\n",
    "        loss='huber',\n",
    "        alpha=5e-10,\n",
    "        epsilon=1.2,\n",
    "        learning_rate='constant',\n",
    "        eta0=0.02,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15, \n",
    "        n_iter_no_change=4,\n",
    "        tol=1e-6,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b32dc8",
   "metadata": {},
   "source": [
    "### 8. Train Model & Predict\n",
    "Fit the model on the training data and generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823f2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred = np.expm1(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eed8d9",
   "metadata": {},
   "source": [
    "### 9. Save Submission\n",
    "Save the predictions to `submission.csv` as required for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab151317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(y_pred).to_csv(\"submission.csv\", index=False, header=True, float_format='%.6f')\n",
    "print(\"submission.csv berhasil dibuat.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
